<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Exercise 1: Loading the Data</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 20px;
        }
        h1, h2 {
            color: #2c3e50;
        }
        code {
            background-color: #f4f4f4;
            padding: 5px;
            border-radius: 3px;
        }
        pre {
            background-color: #f4f4f4;
            padding: 10px;
            border-left: 3px solid #ccc;
            white-space: pre-wrap;
            word-wrap: break-word;
        }
        a {
            color: #2980b9;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
    </style>
</head>
<body>

<h1>Exercise 1: Loading the Data</h1>

<p>In this exercise, we will load the <code>goodreads.csv</code> file, explore the data, and rename the columns to specific names.</p>

<h2>Code:</h2>

<ul>
    <li>Load the <code>goodreads.csv</code> file into Python using Pandas.</li>
    <li>Explore the dataset by printing the first and last 5 rows.</li>
    <li>Rename the columns of the dataset to:
        <pre><code>["rating", "review_count", "isbn", "booktype", "author_url", "year", "genre_urls", "dir", "rating_count", "name"]</code></pre>
    </li>
</ul>
<p>
     <pre><code># 1a loading the csv data set into the notebook
import pandas as pd
f = 'data/goodreads.csv'
df = pd.read_csv(f)

# 1b looking at the frist 5 rows of the dataframe
df.head(5)

# 1b looking at the last 5 rows of the dataframe
df.tail(5)

# 1c adding column names to the 'goodreads.csv' dataframe
df.columns = ["rating", 'review_count', 'isbn', 'booktype','author_url', 'year', 'genre_urls', 'dir','rating_count', 'name']

# 1c testing the dataframe to see if the new column names were added correctly 
df.head(5)</code></pre>
</p>

    <h1>Exercise 2: Subsetting the data</h1>

<ul><li>Subset the data by creating new dataframe only with `["rating", 'isbn', 'author_url', 'year', 'genre_urls', 'name']`</li></ul>
    
<h2>Code:</h2>
    <p><pre><code>
# 2 slicing the pandas dataframe by using the 'loc' or 'iloc' function
# The difference between the loc and iloc functions is that the loc function selects rows using row labels (e.g. tea),
# whereas the iloc function selects rows using their integer positions (staring from 0 and going up by one for each row)
# loc uses the information in the datasets's indices to do its work 
filtered_df = df.loc[0:, ['rating', 'isbn', 'author_url', 'year', 'genre_urls', 'name']]
filtered_df

        # 2 testing the new dataframe to see whether the right columns were transfered out of the original df
filtered_df.head(5)
        </code></pre>
</p>

     <h1>Exercise 3: Manage Missing Data</h1>
<p>Weâ€™ve got a number of ways in general of dealing with missing data. These involve

1. Dropping off cases (or rows) in the data with any missing variables
2. Excluding variables in the data with any missing data 
3. Selectively choosing indicators with only a limited amount of missing data
4. Replacing missing variables with averages, or other representative values
5. Creating a separate model to predict missing data

- Count the missing values in each column
- Manage the missing values (delete or replace values or leave them as they are) and briefly explain your choice for each column</p>

<h2>Code:</h2>
    <p><pre><code>
        # 3a.1 filtered_df has a total of 5998 + 1 rows as shown in exercise 2 
# .count() calcultes the amount of times an 'object' apears in a list (in this case a value)
# so you can subtract the amount of times a value is shown in a row by the total number of rows to find how many rows do NOT have a value (missing values)
5999 - filtered_df.count() 

        # 3a.1 proof that there are 7 missing values in the column 'year' as shown in the outcome of the previous calculation
filtered_df[filtered_df.year.isnull()]

        # 3a.2 can also be done by using .isnull() (to show how many missing values there are) and .sum() to calculate the total per column
filtered_df.isnull().sum()

        #3a.2 which can be writen in a more extensive way as 
missing_values = filtered_df.isnull().sum()
print(missing_values)

        # 3b we checked the data and there are two rows that all miss data in the columns 'rating', 'author_url', 'name'
# we decided to drop those two rows as there are only 2 of them out of 5999 
# but if wanted you could calculate a mean for the numeric data, by doing:  
# "filtered_df['rating'] = filtered_df['rating'].fillna(filtered_df['rating'].mean())"

filtered_df = filtered_df.dropna(subset=['rating'])

# for the column 'isbn' there are 477 missing values, but can be left as is:
# we don't want to alter the data and mess up the curent integers in the column, if needed for further research

# for the column 'year' there are 7 missing values, but can be left as is:
# there are only 7 rows out of 5997, so the missing values won't be that significant, 
# but the rest of the values in that row might be, hence we leave them in 

# for the column 'genre_url' there are 62 missing values, but can be left as is:
# as they make up 1% of the total data set, so the missing values won't be that significant
# but the rest of the values in that row might be, hence we leave them in 

        # 3b test to see if there are any rows left without value for the column 'rating'
filtered_df.loc[filtered_df.rating.isnull()]

        # 3b test to see only two rows were deleted out of the filtered_df dataframe
filtered_df
    </code></pre></p>


    
     <h1>Exercise 4: Shape the data</h1>
<p> - Parse the `author_url` to create new column named `author`
 - Sort the data by putting higher rates go first. If there are overlapping rates, try to put earlier years go first.
 - **(Stretch Goal)** Examine how many books were published at each year and find lowest, highest rate of each year. </p>

<h2>Code:</h2>
    <p><pre><code>
# 4a checking data tyoe of all columns
filtered_df.dtypes

# 4a one of the author_url strings 
author_name = filtered_df.author_url[1]
author_name

# 4a split('/') is used to split the string (the author_url) at every '/' 
# (negative) numbers can be used in selection, int his case you are interested in the last part of the url hence the [-1]
# This will start counting forwards from the end of the values. 
# or you can write it as the [5], if you start counting from the front

author_name.split('/')[-1]
# or 
author_name.split('/')[5]

# 4a .split('.')[1:][0] is added to further split the author_url
# where .split('.') is used to seperate the name from the number 
# and [1,0] was used to slice it and simultaneously remove the first element 
# and [0] was used to extract the first (and left over) element of that newest split

# BUT to make the code more efficient you can also write just [1], 
# as it will immediatly grab just the authors name by telling the program to grab at index 1,
# which in coloqueal language means the second one
author_name.split('/')[-1].split('.')[1]

# 4a use def to creat a function (a block of code which only runs when it is called), its named get_author and (url) is the argument 
# an argument is the value that is sent to the function when it is called
# inside the function the url is split as explained in the previous step
# the function returns to the extrated name
# filtered_df['author'] = filtered_df.author_url.map(get_author) applies the function to each value in the author_url column 
# it also maps the result of the fuunction to a new coloumn named author 

def get_author(url):

    name = url.split('/')[-1].split('.')[1:][0]
     
    return name

filtered_df['author'] = filtered_df.author_url.map(get_author)
filtered_df

# 4b question sort the data by putting higher rates go first. If there are overlapping rates, try to put earlier years go first.
# sort_values(by='rating') was used to sort the dataframe based on the column rating and (ascending=False) was added to start with the highest values

filtered_df.sort_values(by='rating', ascending=False)

# 4b when trying to first sort based on rating and then based on year the code changes
# first of all the () are replaced with [], as the code is now going trough multiple columns and passing a sequence of values 
# [] is used to pass a list, when sorting through multiple columns, a list can hold multiple values 
# so now there are two lists with column names "['rating', 'year']" and "[False, False]"
# "inplace=True" makes the changes directly into the dataframe without needing to reassign it 

filtered_df.sort_values(by=['rating', 'year'], ascending=[False, False], inplace=True)
filtered_df

# 4c **(Stretch Goal)** Examine how many books were published at each year and find lowest, highest rate of each year. 
# filtered_df.groupby('year') was used to group the dataframe by the 'year' column
# `agg()`, which lets you run a bunch of different functions on your DataFrame simultaneously
# book_count=('rating', 'size'), 'size' is used to calculate how many books exist per year (rows in each group)  
# lowest_rating=('rating', 'min'), 'min' is used to find the lowest number in the rating column for each year 
# highest_rating=('rating', 'max') 'max' is used to find the highest number in the rating column for each year
# ).reset_index() this resets the index of the dataframe to amke year a regular column instead of the index  

# note to myself function calls use parentheses (), tuples also us parentheses ()
# lists and dictionaries use square brackets [] and curly braces {}
# inside the agg() function dictionaries are defined as tuples 
# in this case 1 the column e.g. 'rating' and 2 the function e.g. 'size'
# creating a column-function pair written as ('rating', 'size')


books_yearly = filtered_df.groupby('year').agg( 
    book_count=('rating', 'size'), 
    lowest_rating=('rating', 'min'),  
    highest_rating=('rating', 'max')  
).reset_index()

books_yearly

# the outcome of this examination shows that the data should have been cleaned differentlty, there are now negative years left in the dataframe
    </code></pre></p>

     <h1>Exercise 5: Saving the results</h1>
<p>
 - Save the cleaned dataframe as 'hw-03-cleaned.csv' in data folder</p>

<h2>Code:</h2>
    <p><pre><code>
        # 5 saving the dataframe as csv
# 'data/hw-03-cleaned.csv' specifies the path where the file will be saved 
# 'index=False' prevents pandas from writing row indicies to the csv file  

filtered_df.to_csv('data/hw-03-cleaned.csv', index=False)     
    </code></pre></p>

    
     <h1>Exercise 6: Investigate the relationship between the number of reviews and the average rating for books in the dataset cleaned-goodreads.csv procided.</h1>
<p> - Calculate the correlation coefficient. Give me a short definition of this coefficient
  - Create a scatter plot showing the relationship between these two features.
  - Based on the plot and the correlation, provide a brief interpretation of the relationship.

Python Tools: Use pandas and numpy for correlation, and matplotlib or seaborn for the scatter plot.</p>

<h2>Code:</h2>
    <p><pre><code>
        # 6a loading the csv data set into the notebook
import pandas as pd
f = 'data/cleaned-goodreads.csv'
df2 = pd.read_csv(f)
df2.head(5)

# 6a calculating the correlation coefficient between the column 'review_count' and 'rating'
# the .corr() function in pandas is used to calculate the correlation coefficient 
# df2['review_count'] accesses the column with the total number of reviews for each book 
# df2['rating'] accesses the column with the average rating for each book
# the .corr() function is aplied to the column "df2['review_count']"
# the second column is the argument to compare the first column with .corr(df2['rating'])
# a f-string was used to increase readability 

correlation_coefficient = df2['review_count'].corr(df2['rating'])
print(f"Correlation coefficient between the columns 'review_count' and 'rating': {correlation_coefficient}")

# If the correlation coefficient is close to +1: It indicates that books with more reviews tend to have higher ratings
# If it is close to -1: It would suggest that books with more reviews tend to have lower ratings
# If it is near O: It means there's no significant linear relationship between the number of reviews and the rating


# 6b create a scatter plot using seaborn sns.scatterplot(x='review_count', y='rating', data=df2)
# customizing the scatter plot can be done by adding (color='pink', marker='*') to the previous code
# it can be further customized by adding labels and regulating the figure size with the use of matplotlib.pyplot

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(8, 6))
sns.scatterplot(x='review_count', y='rating', data=df2, color='pink', marker='*')

plt.title('Relationship Between Number of Reviews and Rating', fontsize=16)
plt.xlabel('Number of Reviews', fontsize=14)
plt.ylabel('Rating', fontsize=14)

plt.show()

#6c when examining both the correlation coefficient and the scatter plot you can see that here is no linear
# relationship between the number of reviews a book has and its average rating
# so, a book having more or fewer reviews doesn't consistently correlate with it being rated higher or lower
    </code></pre></p>

    <img src = "scatter plot.png"> 
    
     <h1>Exercise 7: Calculate the following descriptive statistics for the numerical features (e.g., number of reviews, average rating, etc.):</h1>
<p>  - Mean
  - Median
  - Standard Deviation
  - Range
  - Create a histogram or box plot for at least one of the numerical features, highlighting any skewness or outliers.
    
Python Tools: Use pandas for data manipulation and matplotlib or seaborn for visualization.</p>

<h2>Code:</h2>
    <p><pre><code>
        # 7a testing the data type per column to help locate the numeric features
df2.dtypes

# 7a a new list was created with all the columns that have numerical values in the dataframe 
# "numerical_features = ['rating', 'review_count', 'year', 'rating_count']"
# to create a more readable print an f-string was used once again 
# in which "\n" is a line break

numerical_features = ['rating', 'review_count', 'year', 'rating_count']
mean_values = df2[numerical_features].mean()
median_values = df2[numerical_features].median()
std_values = df2[numerical_features].std()
range_values = df2[numerical_features].max() - df[numerical_features].min()

print(f"Mean values:\n{mean_values}\n")
print(f"Median values:\n{median_values}\n")
print(f"Standard Deviation:\n{std_values}\n")
print(f"Range:\n{range_values}\n")

# 7b created a histogram for 'rating' using "sns.histplot(df['rating'])"
# bins=20 was added to divide the entire range of values into a series of intervals
# a kernel density estimate was added "kde=True"
# labels are added with the use of matplotlib.pyplot

plt.figure(figsize=(10, 6))
sns.histplot(df['rating'], bins=20, kde=True, color='pink') 

plt.title('Histogram of Book Ratings')
plt.xlabel('Rating')
plt.ylabel('Frequency')
plt.show()
    </code></pre></p>
        <img src = "histogram.png"> 
        <code><pre><p>
# 7b created a boxplot for 'rating' using "sns.boxplot(x=df['rating'])"
# labels are added with the use of matplotlib.pyplot

plt.figure(figsize=(10, 6))
sns.boxplot(x=df['rating'], color='pink')

plt.title('Box Plot of Book Ratings')
plt.xlabel('Rating')
plt.show()

    </code></pre></p>
<img src = "box plot.png"> 
</body>
</html>





